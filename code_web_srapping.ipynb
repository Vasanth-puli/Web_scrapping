{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bbdb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "# Make a GET request to fetch the HTML content\n",
    "url =\"https://insights.blackcoffer.com/difference-between-artificial-intelligence-machine-learning-statistics-and-data-mining/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Extract the article title from the HTML structure\n",
    "title_element = soup.find(\"h1\", class_=\"entry-title\")\n",
    "title = title_element.get_text() if title_element else \"Title not found\"\n",
    "\n",
    "# Tokenize and tag the words in the title\n",
    "tokens = word_tokenize(title)\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "# Print the extracted title and tagged tokens\n",
    "print(\"Title:\", title)\n",
    "\n",
    "paragraphs = soup.select('article p')\n",
    "\n",
    "# Extract the text from each paragraph and join them together\n",
    "body_text = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "# Print the extracted body text\n",
    "print(\"\\nBodytext:\",body_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc54d014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_words=title+ \" \" + body_text\n",
    "final_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb185cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "cleaned_text = ' '.join(word for word in final_words.split() if word.lower() not in stop_words)\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ff98c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d932ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(cleaned_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f765a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d04450",
   "metadata": {},
   "outputs": [],
   "source": [
    "pure=word_tokenize(cleaned_text)\n",
    "pure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc5f37b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('opinion_lexicon')\n",
    "from nltk.probability import FreqDist\n",
    "positive_words = set(opinion_lexicon.positive())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4664058f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "positive_tokens = [token for token in pure if token in positive_words]\n",
    "freq_dist = FreqDist(positive_tokens)\n",
    "\n",
    "for word, count in freq_dist.items():\n",
    "    print(word, \":\", count)\n",
    "sum_positive_counts = sum(freq_dist.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8480490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sum of Positive Word Counts:\", sum_positive_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08f7967",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words = set(opinion_lexicon.negative())\n",
    "negative_tokens = [token for token in pure if token in negative_words]\n",
    "freq_dist = FreqDist(negative_tokens)\n",
    "\n",
    "for word, count in freq_dist.items():\n",
    "    print(word, \":\", count)\n",
    "    sum_negative_counts = sum(freq_dist.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbac8281",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sum of Negative Word Counts:\", sum_negative_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fd4452",
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_score = (sum_positive_counts - sum_negative_counts) / (sum_positive_counts + sum_negative_counts + 0.000001)\n",
    "print(\"polarity score=\",polarity_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f14391",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = len(final_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50701224",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectivity_score = (sum_positive_counts + sum_negative_counts) / (total_words + 0.000001)\n",
    "print(\"subjectivity score=\",subjectivity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pure\n",
    "word_counts = [len(word_tokenize(sentence)) for sentence in sentences]\n",
    "average_sentence_length = sum(word_counts) / len(sentences)\n",
    "print(\"Average sentence length=\",average_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f75647",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = cmudict.dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e5afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word):\n",
    "    return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_word_count = sum(1 for token in final_words if token.strip() and token.strip() in d and count_syllables(token.strip()) > 2)\n",
    "total_words = len(final_words)\n",
    "percentage_complex_words = (complex_word_count / total_words) * 100\n",
    "\n",
    "print(\"percentage complex words=\",percentage_complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c409f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(cleaned_text)\n",
    "total_sentences = len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b698ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_sentence_length = total_words / total_sentences\n",
    "percentage_complex_words = (complex_word_count / total_words) * 100\n",
    "fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
    "\n",
    "print(\"Fog Index:\", fog_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3169d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_words_per_sentence = total_words / total_sentences\n",
    "print(\"Average words per sentence=\",average_words_per_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7519b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"complex word count=\",complex_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = len(cleaned_text)\n",
    "print(\"Word count=\",word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d2090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def count_syllables(word):\n",
    "    # Handling exceptions for words ending with \"es\" or \"ed\"\n",
    "    if word.endswith((\"es\", \"ed\")):\n",
    "        word = word[:-2]\n",
    "    \n",
    "    # Counting the number of vowels in the word\n",
    "    syllables = len(re.findall(r'[aeiouyAEIOUY]+', word))\n",
    "    \n",
    "    return syllables\n",
    "\n",
    "syllable_count = sum(count_syllables(word) for word in final_words)\n",
    "\n",
    "print(\"Sum of Syllable Counts:\", syllable_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fbeff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
    "regex_pattern = r'\\b(?:{})\\b'.format('|'.join(personal_pronouns))\n",
    "\n",
    "text = ' '.join(cleaned_text)\n",
    "\n",
    "matches = re.findall(regex_pattern, text, re.IGNORECASE)\n",
    "count_personal_pronouns = len(matches)\n",
    "\n",
    "print(\"Count of Personal Pronouns:\", count_personal_pronouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d70cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_characters = sum(len(word) for word in cleaned_text)\n",
    "total_words = len(final_words)\n",
    "\n",
    "average_word_length = total_characters / total_words\n",
    "\n",
    "print(\"Average Word Length:\", average_word_length)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
